---
title: "Homework 7"
author: "Ryan Kingery"
date: "10/25/2017"
output:
  html_document: default
  pdf_document: default
header-includes:
- \usepackage{amsmath}
- \usepackage{amsfonts}
- \usepackage{amssymb}
- \usepackage{tikz}
- \usepackage{amsthm}
---

```{r setup, include=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(parallel)
library(foreach)
library(doParallel)
library(knitr)
```

# Problem 2
The following code reproduces the non-parallel methods used last time. A large vector $y$ is initiated from a random Gaussian, its mean is calculated, and its SSE is calculated using first a for loop and second a vectorized approach.
```{r, echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(comment=FALSE),eval = FALSE}
set.seed(12345)
y <- seq(from=1, to=100, length.out = 1e8) + rnorm(1e8)
ybar <- mean(y)

system.time({
  sse_loop <- 0
  for (i in 1:length(y)) {
    sse_loop <- sse_loop + (y[i]-ybar)^2
  }
})

system.time({
  sse_vect <- sum((y-ybar)^2)
})
```

My attempts at using the parallel methods are shown below. The first attempt uses the foreach function, and on my computer generates a process that won't halt. The second attempt uses the parSapply function and generates mysterious errors.
```{r, echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(comment=FALSE),eval = FALSE}
sse_loop <- 0
update_sse <- function(yi) {sse_loop <- sse_loop + (yi-ybar)^2}
cl <- makeCluster(detectCores()-1,type="FORK")
clusterExport(cl,"sse_loop")
clusterExport(cl,"ybar")
system.time({
  foreach(i=1:length(y),.combine=data.frame) %dopar%
    update_sse(yi)
})
stopCluster(cl)

sse_loop <- 0
cl <- makeCluster(detectCores()-1,type="FORK")
clusterExport(cl,"sse_loop")
clusterExport(cl,"ybar")
system.time({
  parSapply(cl,as.data.frame(y),function(x) {sse_loop <- sse_loop + x; sse_loop})
})
stopCluster(cl)
```


# Problem 3
The following code initializes a design matrix $X$ and responses $y$ at random. Values for the model parameters $\theta$ are initialized as well, along with the learning rate $\alpha$. Note that $\alpha$ is initialized as a grid since this is what we desire to parallelize over to see how differing learning rates affect convergence of the algorithm.
```{r, echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(comment=FALSE),eval = FALSE}
set.seed(1256)
alpha = c(0.001,0.01,0.1,1.0,10.0,100.0)
theta_prev <- as.matrix(rnorm(2,0,1),nrow=2)
theta <- as.matrix(c(1,2),nrow=2)
X <- cbind(1,rep(1:10,10))
y <- X%*%theta+rnorm(100,0,0.2)
m <- length(y)
```

Next, we implement a function to perform gradient descent. The function inputs $X$, $y$, $\theta$, and $\alpha$. Two optional arguments, the stopping tolerance and the number of iterations to run, are accepted as well, but we will use the preset values. The function is set to halt when the tolerance is exceeded, or at worst when the iteration number has been met, and returns the (locally) optimal values of $\theta$ assuming the inputs are well specified.
```{r, echo=TRUE, message=FALSE, warning=FALSE, list(comment=FALSE), tidy=TRUE, tidy.opts=list(comment=FALSE),eval = FALSE}
gradient_descent <- function(theta,X,y,alpha,tol=10e-4,iters=1000) {
  theta_prev <- as.matrix(rnorm(2,0,1),nrow=2)
  m <- length(y)
  for (i in 1:iters) {
    theta_prev <- theta
    theta[1,1] <- theta[1,1]-alpha*1/m*sum((theta[1,1]+theta[2,1]*X[,2]-y))
    theta[2,1] <- theta[2,1]-alpha*1/m*sum((theta[1,1]+theta[2,1]*X[,2]-y)*X[,2])
    if ((abs(theta[1,1]-theta_prev[1,1]) & abs(theta[2,1]-theta_prev[2,1]))<tol) {
      break
    }
  }
  return(theta)
}
```

A failed attempt is now made to apply the parSapply function on $\alpha$ using gradient descent as the input function. Unfortunately, mysterious errors are preventing me from getting a good analysis of the outputs or assessing the run time.
```{r, echo=FALSE, message=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(comment=FALSE),eval = FALSE}
cl <- makeCluster(detectCores()-1,type="FORK")
clusterExport(cl,"theta")
clusterExport(cl,"X")
clusterExport(cl,"y")
system.time({
  parSapply(cl,as.data.frame(alphas),function(x) gradient_descent(theta,X,y,alphas))
})
stopCluster(cl)
```


# Problem 4
In this problem we will use bootstrapping to estimate the distributions of the parameters $\beta$ in the usual linear model $y = X\beta+\epsilon$, given a design matrix $X$ and a response vector $y$. The values needed are initialized below. For comparison, the known values of $\beta$ are specified as well.
```{r, echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(comment=FALSE)}
set.seed(1267)
n <- 200
X <- 1/cbind(1,rt(n,df=1),rt(n,df=1),rt(n,df=1))
beta <- c(1,2,3,0)
y <- X %*% beta + rnorm(100,sd=3)
```

We now perform a bootstrap over 10,000 iterations. This is done by combining $X,y$ into a matrix $Z$ and sampling with replacement from $Z$ exactly 200 times on each iteration. The sampled values of $\beta$ are then calculated using the lm function and stored in a dataframe for later use.
```{r, echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(comment=FALSE)}
B <- 10000
Z <- cbind(X,y)
betas <- data.frame(0*1:B,0*1:B,0*1:B,0*1:B)
names(betas) <- c("B0","B1","B2","B3")
for (b in 1:B) {
  Zb <- Z[sample(nrow(X),nrow(X),replace=TRUE),]
  Xb <- Zb[,1:4]
  yb <- Zb[,5]
  betas[b,] <- coef(lm(yb~Xb[,2:4]))
}
```

We can now plot the distributions of $\beta$ using these bootstrap estimates.
```{r, echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(comment=FALSE)}
par(mfrow=c(2,2))
hist(betas$B0,main="beta_0",xlab="beta_0")
hist(betas$B1,main="beta_1",xlab="beta_1")
hist(betas$B2,main="beta_2",xlab="beta_2")
hist(betas$B3,main="beta_3",xlab="beta_3")
```

To verify that the boostrap distributions give good point estimates of the actual $\beta$ values specified above, we can calculate the column means of the dataframe:
```{r, echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(comment=FALSE)}
means <- sapply(betas,mean)
stdevs <- sapply(betas,sd)
kable(t(as.data.frame(cbind(means,stdevs))))
```

As far as performing the bootstrap above in parallel, I'll refrain from trying to code it up since it's not working well for me at all. However, if one wanted to, perhaps one could parallelize over the bootstrap iterations since they're independent of each other.
